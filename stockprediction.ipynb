{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import arch\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to preprocess and split data\n",
    "def preprocess_data(data):\n",
    "    data.reset_index(inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    data['LTTS.NS100'] = data['Close'].rolling(100).mean()\n",
    "    data['LTTS.NS200'] = data['Close'].rolling(200).mean()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_data(train_data, test_data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train_data[['Close']])\n",
    "    test_scaled = scaler.transform(test_data[['Close']])\n",
    "    return scaler, train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(data, look_back=100):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:(i + look_back), 0])\n",
    "        Y.append(data[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train neural network models\n",
    "def train_neural_network(x_train, y_train):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train LSTM model\n",
    "def train_lstm(x_train, y_train):\n",
    "    LSTM_model = Sequential()\n",
    "    LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "    LSTM_model.add(LSTM(units=50))\n",
    "    LSTM_model.add(Dense(units=1))\n",
    "    LSTM_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    LSTM_model.fit(x_train, y_train, epochs=50, batch_size=32)\n",
    "    return LSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save models\n",
    "def save_model(model, filename):\n",
    "    model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and save Random Forest and Gradient Boosting models\n",
    "def train_and_save_ensemble_models(x_train, y_train):\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(x_train, y_train)\n",
    "    with open('artifacts/random_forest_regressor_model.pkl', 'wb') as f:\n",
    "        pickle.dump(rf_model, f)\n",
    "\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    gb_model.fit(x_train, y_train)\n",
    "    with open('artifacts/gradientboost_regressor_model.pkl', 'wb') as f:\n",
    "        pickle.dump(gb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "def make_predictions(model, x_test):\n",
    "    return model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_models(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mae, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train GARCH model\n",
    "def train_garch_model(data):\n",
    "    garch_model = arch.arch_model(data['Close'], vol='GARCH', p=1, q=1)\n",
    "    garch_res = garch_model.fit(update_freq=5)\n",
    "    garch_volatility = garch_res.conditional_volatility\n",
    "    return garch_res, garch_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot original vs predicted prices\n",
    "def plot_predictions(test_data, y_test_padded, y_pred_models, model_names=None):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=test_data['Date'], y=y_test_padded.flatten(), mode='lines', name='Original Price', line=dict(color='black')))\n",
    "    colors = ['red', 'green', 'purple', 'darkblue','orange']\n",
    "    if model_names is None:\n",
    "        model_names = ['Model 1', 'Model 2', 'Model 3', 'Model 4']\n",
    "    for i, y_pred in enumerate(y_pred_models, start=1):\n",
    "        fig.add_trace(go.Scatter(x=test_data['Date'].iloc[100:], y=y_pred.flatten(), mode='lines', name=model_names[i-1], line=dict(color=colors[i-1])))\n",
    "    fig.update_layout(title='Original vs Predicted Prices', xaxis_title='Date', yaxis_title='Price')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input for stock ticker symbol\n",
    "stock_symbol = st.sidebar.text_input('Enter Stock Ticker Symbol (e.g., MSFT):')\n",
    "\n",
    "# Date range input\n",
    "start_date = st.sidebar.date_input('Select Start Date:', datetime.now() - timedelta(days=365))\n",
    "end_date = st.sidebar.date_input('Select End Date:', datetime.now())\n",
    "\n",
    "    \n",
    "# Define stock data retrieval parameters\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2024-02-01'\n",
    "stock_symbol = 'LTTS.NS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning:\n",
      "\n",
      "You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "43/43 [==============================] - 7s 63ms/step - loss: 0.0127\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 2s 58ms/step - loss: 9.6599e-04\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 3s 58ms/step - loss: 8.8987e-04\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 7.9209e-04\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 3s 76ms/step - loss: 8.0165e-04\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 4s 83ms/step - loss: 6.9138e-04\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 3s 80ms/step - loss: 6.4670e-04\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 3s 79ms/step - loss: 6.3515e-04\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 4s 94ms/step - loss: 6.2511e-04\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 4s 87ms/step - loss: 7.1695e-04\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 6.0196e-04\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 3s 69ms/step - loss: 5.7251e-04\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 5.0615e-04\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 4.7919e-04\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 3s 76ms/step - loss: 4.5461e-04\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 4s 98ms/step - loss: 4.7138e-04\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 4s 89ms/step - loss: 4.4612e-04\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 4s 82ms/step - loss: 5.7615e-04\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 3s 74ms/step - loss: 4.0363e-04\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 4.0891e-04\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 4.2844e-04\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 3s 67ms/step - loss: 3.7353e-04\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 3.8604e-04\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 3s 72ms/step - loss: 4.0285e-04\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 3.7392e-04\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 3.9457e-04\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 2s 56ms/step - loss: 3.6105e-04\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 3.6504e-04\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 3.2919e-04\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 3s 61ms/step - loss: 3.2617e-04\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 3s 75ms/step - loss: 3.1477e-04\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 3s 75ms/step - loss: 3.0699e-04\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 3.0723e-04\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 4.0381e-04\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 2.8809e-04\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 3.1331e-04\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 3.0614e-04\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 3s 66ms/step - loss: 2.7888e-04\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 2.7664e-04\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 3s 60ms/step - loss: 2.7583e-04\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 2s 53ms/step - loss: 2.7190e-04\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 3s 65ms/step - loss: 2.6816e-04\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 2.6285e-04\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 3s 63ms/step - loss: 2.7177e-04\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 2.9685e-04\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 2s 57ms/step - loss: 2.7399e-04\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 3s 59ms/step - loss: 2.7011e-04\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 3s 62ms/step - loss: 2.9276e-04\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 2s 54ms/step - loss: 3.4258e-04\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 2s 55ms/step - loss: 2.4473e-04\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 1s 22ms/step\n",
      "Iteration:      5,   Func. Count:     28,   Neg. LLF: 15308.082903448008\n",
      "Iteration:     10,   Func. Count:     53,   Neg. LLF: 72628.36262770054\n",
      "Iteration:     15,   Func. Count:     83,   Neg. LLF: 15282.714393904096\n",
      "Iteration:     20,   Func. Count:    111,   Neg. LLF: 14800.002296301875\n",
      "Iteration:     25,   Func. Count:    136,   Neg. LLF: 14412.205455897278\n",
      "Iteration:     30,   Func. Count:    161,   Neg. LLF: 14411.399471881272\n",
      "Iteration:     35,   Func. Count:    186,   Neg. LLF: 14411.391212733033\n",
      "Iteration:     40,   Func. Count:    211,   Neg. LLF: 14410.33210488145\n",
      "Iteration:     45,   Func. Count:    236,   Neg. LLF: 14342.017528434071\n",
      "Iteration:     50,   Func. Count:    266,   Neg. LLF: 14318.223797670367\n",
      "Iteration:     55,   Func. Count:    296,   Neg. LLF: 14109.992672397046\n",
      "Iteration:     60,   Func. Count:    321,   Neg. LLF: 14109.522087680747\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 14109.522087684454\n",
      "            Iterations: 61\n",
      "            Function evaluations: 336\n",
      "            Gradient evaluations: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\arch\\univariate\\base.py:311: DataScaleWarning:\n",
      "\n",
      "y is poorly scaled, which may affect convergence of the optimizer when\n",
      "estimating the model parameters. The scale of y is 2.037e+06. Parameter\n",
      "estimation work better when this value is between 1 and 1000. The recommended\n",
      "rescaling is 0.01 * y.\n",
      "\n",
      "This warning can be disabled by either rescaling y before initializing the\n",
      "model or by setting rescale=False.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12772\\646066957.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# Plot predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mplot_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_models\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;31m# plot_predictions(test_data, np.concatenate((np.zeros((100, 1)), y_test)), y_pred_models_with_garch, model_names=['Neural Network (with GARCH)', 'LSTM (with GARCH)', 'Random Forest (with GARCH)', 'Gradient Boosting (with GARCH)'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12772\\3521808500.py\u001b[0m in \u001b[0;36mplot_predictions\u001b[1;34m(test_data, y_test_padded, y_pred_models, model_names)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmodel_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Model 1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Model 2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Model 3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Model 4'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lines'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Original vs Predicted Prices'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxaxis_title\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myaxis_title\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Price'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Load stock data\n",
    "if stock_symbol:\n",
    "    stock_data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "    st.subheader('Stock Data')\n",
    "    st.write(stock_data.head(50))  # Display first 50 rows\n",
    "\n",
    "\n",
    "    # Download stock data\n",
    "    data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "    data = preprocess_data(data)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    train_data = data[:train_size]\n",
    "    test_data = data[train_size:]\n",
    "\n",
    "    scaler, train_scaled, test_scaled = scale_data(train_data, test_data)\n",
    "\n",
    "    x_train, y_train = create_dataset(train_scaled)\n",
    "    x_test, y_test = create_dataset(test_scaled)\n",
    "\n",
    "    # Train and save neural network models\n",
    "    nn_model = train_neural_network(x_train, y_train)\n",
    "    save_model(nn_model, 'artifacts/NN_model.h5')\n",
    "\n",
    "    # Train and save LSTM model\n",
    "    lstm_model = train_lstm(x_train, y_train)\n",
    "    save_model(lstm_model, 'artifacts/LSTM_model.h5')\n",
    "\n",
    "    # Train and save Random Forest and Gradient Boosting models\n",
    "    train_and_save_ensemble_models(x_train, y_train)\n",
    "\n",
    "    # # GARCH model\n",
    "    # garch_model = arch.arch_model(train_data['Close'], vol='GARCH', p=1, q=1)\n",
    "    # garch_res = garch_model.fit(update_freq=5)\n",
    "    # garch_volatility = garch_res.conditional_volatility\n",
    "\n",
    "    # Make predictions\n",
    "    nn_pred = make_predictions(nn_model, x_test)\n",
    "    lstm_pred = make_predictions(lstm_model, x_test)\n",
    "\n",
    "    with open('artifacts/random_forest_regressor_model.pkl', 'rb') as f:\n",
    "        rf_model = pickle.load(f)\n",
    "    rf_pred = make_predictions(rf_model, x_test)\n",
    "\n",
    "    with open('artifacts/gradientboost_regressor_model.pkl', 'rb') as f:\n",
    "        gb_model = pickle.load(f)\n",
    "    gb_pred = make_predictions(gb_model, x_test)\n",
    "\n",
    "    # Train GARCH model\n",
    "    garch_res, garch_volatility = train_garch_model(data)\n",
    "        \n",
    "    # Save the trained GARCH model using pickle\n",
    "    with open('artifacts/garch_model.pkl', 'wb') as f:\n",
    "        pickle.dump(garch_res, f)\n",
    "\n",
    "    # Reshape GARCH volatility for compatibility with other predictions\n",
    "    garch_volatility = garch_volatility.values.reshape(-1, 1)\n",
    "\n",
    "    # # Repeat or interpolate garch_volatility to match the size of x_test\n",
    "    # garch_volatility_resized = np.repeat(garch_volatility, len(x_test) // len(garch_volatility) + 1)[:len(x_test)]\n",
    "\n",
    "    # # Reshape garch_volatility to match the shape of x_test\n",
    "    # garch_volatility_resized = garch_volatility_resized.reshape(-1, 1)\n",
    "\n",
    "    # # Concatenate garch_volatility with other features\n",
    "    # x_test_with_garch = np.concatenate((x_test, garch_volatility_resized), axis=1)\n",
    "\n",
    "    # # Make predictions using modified features\n",
    "    # nn_pred_with_garch = make_predictions(nn_model, x_test_with_garch)\n",
    "    # lstm_pred_with_garch = make_predictions(lstm_model, x_test_with_garch)\n",
    "    # rf_pred_with_garch = make_predictions(rf_model, x_test_with_garch)\n",
    "    # gb_pred_with_garch = make_predictions(gb_model, x_test_with_garch)\n",
    "\n",
    "    # Inverse transform the GARCH model predictions\n",
    "    garch_pred = scaler.inverse_transform(garch_volatility.reshape(-1, 1))\n",
    "\n",
    "    # Inverse transform the predictions and actual values\n",
    "    y_pred_models = [scaler.inverse_transform(pred.reshape(-1, 1)) for pred in [nn_pred, lstm_pred, rf_pred, gb_pred,garch_pred]]\n",
    "    # y_pred_models_with_garch = [scaler.inverse_transform(pred.reshape(-1, 1)) for pred in [nn_pred_with_garch, lstm_pred_with_garch, rf_pred_with_garch, gb_pred_with_garch]]\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_predictions(test_data, np.concatenate((np.zeros((100, 1)), y_test)), y_pred_models)\n",
    "    # plot_predictions(test_data, np.concatenate((np.zeros((100, 1)), y_test)), y_pred_models_with_garch, model_names=['Neural Network (with GARCH)', 'LSTM (with GARCH)', 'Random Forest (with GARCH)', 'Gradient Boosting (with GARCH)'])\n",
    "\n",
    "    # Evaluate models\n",
    "    mae_mse = [evaluate_models(y_test, y_pred) for y_pred in y_pred_models]\n",
    "    # mae_mse_with_garch = [evaluate_models(y_test, y_pred) for y_pred in y_pred_models_with_garch]\n",
    "\n",
    "    st.write('Without GARCH:')\n",
    "    st.write(f'Neural Network: MAE={mae_mse[0][0]}, MSE={mae_mse[0][1]}')\n",
    "    st.write(f'LSTM: MAE={mae_mse[1][0]}, MSE={mae_mse[1][1]}')\n",
    "    st.write(f'Random Forest: MAE={mae_mse[2][0]}, MSE={mae_mse[2][1]}')\n",
    "    st.write(f'Gradient Boosting: MAE={mae_mse[3][0]}, MSE={mae_mse[3][1]}')\n",
    "\n",
    "    # Display GARCH model summary\n",
    "    st.subheader('GARCH Model Summary')\n",
    "    st.write(garch_res.summary())\n",
    "\n",
    "# st.write('With GARCH:')\n",
    "# st.write(f'Neural Network (with GARCH): MAE={mae_mse_with_garch[0][0]}, MSE={mae_mse_with_garch[0][1]}')\n",
    "# st.write(f'LSTM (with GARCH): MAE={mae_mse_with_garch[1][0]}, MSE={mae_mse_with_garch[1][1]}')\n",
    "# st.write(f'Random Forest (with GARCH): MAE={mae_mse_with_garch[2][0]}, MSE={mae_mse_with_garch[2][1]}')\n",
    "# st.write(f'Gradient Boosting (with GARCH): MAE={mae_mse_with_garch[3][0]}, MSE={mae_mse_with_garch[3][1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
